### Основные функции

- **Обучение модели** с несколькими эпохами (поколениями)
- **Асинхронное предсказание** для высокой производительности
- **Управление версиями моделей** - поддержка нескольких версий одновременно
- **Автоматическое сохранение** обученных моделей
- **Метрики обучения** - отслеживание loss и accuracy

### Архитектура модели

Нейронная сеть состоит из:
- **Входной слой**: 8 признаков транзакции
- **Скрытые слои**: [64, 32] нейронов с ReLU активацией
- **Dropout**: 0.2 для предотвращения переобучения
- **Выходной слой**: 1 нейрон с sigmoid активацией (вероятность фрода 0-1)

### Признаки транзакции

**Модель использует те же признаки, что и основное приложение:**

1. **amount** - сумма транзакции (нормализованная 0-1, деление на 1,000,000)
2. **hourOfDay** - час дня (0-23, нормализованный до 0-1)
3. **dayOfWeek** - день недели (1-7, нормализованный до 0-1)
4. **accountFromHash** - хеш счета отправителя (нормализованный 0-1)
5. **accountToHash** - хеш счета получателя (нормализованный 0-1)
6. **transactionType** - тип транзакции (категориальный 0-1):
   - TRANSFER = 0.25
   - PAYMENT = 0.5
   - WITHDRAWAL = 0.75
   - DEPOSIT = 1.0
7. **ipRiskScore** - оценка риска IP адреса (0-1)
8. **locationRisk** - оценка риска локации (0-1)

## Процесс обучения

1. **Инициализирует** веса нейронной сети
2. **Проходит несколько эпох** обучения
3. **Вычисляет loss** с использованием sigmoid binary cross-entropy
4. **Обновляет веса** с помощью Adam optimizer
5. **Отслеживает метрики** для каждой эпохи
6. **Сохраняет модель** после обучения

### Пример вывода обучения

```
=== Starting Training ===
Training samples: 1000
Epochs: 50
Batch size: 32
Learning rate: 0.001

Epoch 1/50 - Loss: 0.6932, Accuracy: 50.00%
Epoch 2/50 - Loss: 0.6847, Accuracy: 58.00%
Epoch 3/50 - Loss: 0.6723, Accuracy: 65.00%
...
Epoch 50/50 - Loss: 0.2145, Accuracy: 92.30%

Model saved to: models
=== Training Complete ===
```

### Синтетические данные

Генератор создает реалистичные паттерны (20% fraud, 80% normal):

**Normal транзакции:**
- Суммы: 0.05-0.45 (умеренные)
- Время: 8-20 часов (дневное)
- IP риск: 0-0.3 (низкий)
- Location риск: 0-0.3 (низкий)
- Типы: PAYMENT, DEPOSIT

**Fraud транзакции:**
- Суммы: 0.3-1.0 (большие)
- Время: 0-7 или 22-24 часов (ночное)
- IP риск: 0.6-1.0 (высокий)
- Location риск: 0.6-1.0 (высокий)
- Типы: WITHDRAWAL, TRANSFER

### Параметры модели

- **inputSize**: 8 (количество признаков)
- **hiddenLayers**: [64, 32] (размеры скрытых слоев)
- **dropout**: 0.2 (процент dropout)
- **activationFunction**: ReLU (скрытые слои), Sigmoid (выходной слой)

### Параметры оптимизатора

- **Optimizer**: Adam
- **Loss Function**: Sigmoid Binary Cross-Entropy
- **Learning Rate**: 0.001 (по умолчанию)

## Технологии

- **Deep Java Library (DJL)** - фреймворк для машинного обучения
- **PyTorch** - бэкенд для нейронных сетей
- **Java 21** - язык программирования
- **Gradle** - система сборки

## Метрики качества

После обучения доступны следующие метрики:

- **Loss по эпохам** - траектория функции потерь
- **Accuracy по эпохам** - точность модели
- **Final Loss** - финальное значение loss
- **Final Accuracy** - финальная точность

## Best Practices

1. **Нормализация данных**: Все признаки должны быть нормализованы в диапазон [0, 1]
2. **Балансировка классов**: Используйте техники балансировки при несбалансированных данных
3. **Validation set**: Разделите данные на train/validation для избежания переобучения
4. **Мониторинг метрик**: Отслеживайте loss и accuracy для выявления переобучения
5. **Early stopping**: Остановите обучение при стагнации метрик

## Отладка

Если модель показывает плохие результаты:

1. Проверьте нормализацию данных
2. Увеличьте количество эпох
3. Попробуйте разные learning rates (0.0001 - 0.01)
4. Измените архитектуру (добавьте слои или нейроны)
5. Добавьте больше данных для обучения

