### Основные функции

- **Обучение модели** с несколькими эпохами (поколениями)
- **Асинхронное предсказание** для высокой производительности
- **Управление версиями моделей** - поддержка нескольких версий одновременно
- **Автоматическое сохранение** обученных моделей
- **Метрики обучения** - отслеживание loss и accuracy

### Архитектура модели

Нейронная сеть состоит из:
- **Входной слой**: 7 признаков транзакции
- **Скрытые слои**: [64, 32] нейронов с ReLU активацией
- **Dropout**: 0.2 для предотвращения переобучения
- **Выходной слой**: 1 нейрон с sigmoid активацией (вероятность фрода)

### Признаки транзакции

1. **amount** - сумма транзакции (нормализованная)
2. **avgUserAmount** - средняя сумма транзакций пользователя
3. **txFrequency** - частота транзакций
4. **hourSin** - синус времени транзакции (циклическая кодировка)
5. **hourCos** - косинус времени транзакции
6. **dayOfWeek** - день недели (нормализованный)
7. **ipRiskScore** - оценка риска IP адреса

## Процесс обучения

1. **Инициализирует** веса нейронной сети
2. **Проходит несколько эпох** обучения
3. **Вычисляет loss** с использованием sigmoid binary cross-entropy
4. **Обновляет веса** с помощью Adam optimizer
5. **Отслеживает метрики** для каждой эпохи
6. **Сохраняет модель** после обучения

### Пример вывода обучения

```
=== Starting Training ===
Training samples: 1000
Epochs: 50
Batch size: 32
Learning rate: 0.001

Epoch 1/50 - Loss: 0.6932, Accuracy: 50.00%
Epoch 2/50 - Loss: 0.6847, Accuracy: 58.00%
Epoch 3/50 - Loss: 0.6723, Accuracy: 65.00%
...
Epoch 50/50 - Loss: 0.2145, Accuracy: 92.30%

Model saved to: models
=== Training Complete ===
```

Синтетические данные создают реалистичные паттерны:
- **Normal транзакции**: типичные суммы, регулярная активность, низкий IP риск
- **Fraud транзакции**: большие суммы, нерегулярная активность, высокий IP риск

### Параметры модели

- **inputSize**: 7 (количество признаков)
- **hiddenLayers**: [64, 32] (размеры скрытых слоев)
- **dropout**: 0.2 (процент dropout)

### Параметры оптимизатора

- **Optimizer**: Adam
- **Loss Function**: Sigmoid Binary Cross-Entropy
- **Learning Rate**: 0.001 (по умолчанию)

## Технологии

- **Deep Java Library (DJL)** - фреймворк для машинного обучения
- **PyTorch** - бэкенд для нейронных сетей
- **Java 21** - язык программирования
- **Gradle** - система сборки

## Метрики качества

После обучения доступны следующие метрики:

- **Loss по эпохам** - траектория функции потерь
- **Accuracy по эпохам** - точность модели
- **Final Loss** - финальное значение loss
- **Final Accuracy** - финальная точность

## Best Practices

1. **Нормализация данных**: Все признаки должны быть нормализованы в диапазон [0, 1]
2. **Балансировка классов**: Используйте техники балансировки при несбалансированных данных
3. **Validation set**: Разделите данные на train/validation для избежания переобучения
4. **Мониторинг метрик**: Отслеживайте loss и accuracy для выявления переобучения
5. **Early stopping**: Остановите обучение при стагнации метрик

## Отладка

Если модель показывает плохие результаты:

1. Проверьте нормализацию данных
2. Увеличьте количество эпох
3. Попробуйте разные learning rates (0.0001 - 0.01)
4. Измените архитектуру (добавьте слои или нейроны)
5. Добавьте больше данных для обучения

